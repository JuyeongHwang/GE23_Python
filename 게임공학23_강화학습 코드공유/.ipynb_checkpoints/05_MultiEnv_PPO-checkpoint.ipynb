{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a406aa4",
   "metadata": {},
   "source": [
    "tensorboad 다운받아야함.  \n",
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import multi_env\n",
    "from SimplePPO import PPO\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "ENVCOUNT = 12\n",
    "multi_env = multi_env(ENVCOUNT)\n",
    "# ** new **\n",
    "agent = PPO(input_dim=4, action_dim=2)\n",
    "\n",
    "episode_history = [[] for x in range(ENVCOUNT)]\n",
    "episode_rewards = [0 for x in range(ENVCOUNT)]\n",
    "print(episode_history)\n",
    "print(episode_rewards)\n",
    "\n",
    "states = []\n",
    "\n",
    "for r in range(ENVCOUNT):\n",
    "    state = multi_env.reset(r)\n",
    "    states.append(state)\n",
    "    # print(f\"id: {r}, state: {state}\")\n",
    "\n",
    "for timestep in range(100000): #timestep\n",
    "    print(f\"=== ep {timestep} ====\")\n",
    "    actions = []\n",
    "    probs = []\n",
    "\n",
    "    for id in range(ENVCOUNT):\n",
    "        prob, action = agent.select_action(states[id])\n",
    "        actions.append(action)\n",
    "        probs.append(prob[action].item())\n",
    "\n",
    "    # print(actions)\n",
    "    n_states, rewards, dones = multi_env.step(actions)\n",
    "\n",
    "    for i in range(ENVCOUNT):\n",
    "        transition = (states[i], actions[i], rewards[i], n_states[i], probs[i], dones[i])\n",
    "        # print(transition)\n",
    "        agent.put_data((states[i], actions[i], rewards[i], n_states[i], probs[i], dones[i]))\n",
    "        episode_rewards[i] += rewards[i]\n",
    "\n",
    "        if(dones[i]):\n",
    "            if(i==0):\n",
    "                writer.add_scalar(\"Reward\", episode_rewards[i], len(episode_history[i]))\n",
    "            multi_env.reset(i)\n",
    "            episode_history[i].append(episode_rewards[i])\n",
    "            episode_rewards[i] = 0\n",
    "\n",
    "    probs.clear()\n",
    "    actions.clear()\n",
    "    states.clear()\n",
    "\n",
    "    states = n_states\n",
    "\n",
    "    if timestep % 300 == 0 and timestep != 0:\n",
    "        print(\"===trained===\")\n",
    "        for r in range(ENVCOUNT):\n",
    "            state = multi_env.reset(r)\n",
    "        agent.train_net()\n",
    "        torch.save(agent.state_dict(), f\"./save_weights/PPO.h5\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
